import torch.nn.functional as F
import numpy as np
import torch.nn.functional as F
import torchvision
import torch
from pytorch_wrapper.functional import masked_mean_pooling
from ctrl.utils.panoptic_deeplab.utils import get_module
import os
from datetime import datetime
from lapsolver import solve_dense


def create_panop_eval_dirs(snapshot_dir, logger):
    # generate folder for panoptic evaluation with a unique name
    # so that everytime when we evalute and then remove the folder and create new root folder and subfolders
    # they don't interfer one another, i mean if you have the sameeval_root folder then once you delte that
    # and create once again the same root folder there should be a time gap as the eval root folder is large
    # in size and it may take few seconds to reomve all the png and text files generated by the evaluation
    # script, so to be in a safer side I generate the eval root folder witha unique name every time
    # so that even if the deletion of the previous eval root folder takes sometime, i can instanly create
    # anothe unique named eval root foler for the next evaluation
    # str1 = datetime.now().strftime("%m-%Y")
    str2 = datetime.now().strftime("%d-%m-%Y")
    str3 = datetime.now().strftime("%H-%M-%S-%f")
    panop_eval_root_folder = os.path.join(snapshot_dir, 'panop_eval_{}_{}'.format(str2, str3))
    panop_eval_folder_dict = {}
    panop_eval_folder_dict['semantic'] = os.path.join(panop_eval_root_folder, 'semantic')
    panop_eval_folder_dict['instance'] = os.path.join(panop_eval_root_folder, 'instance')
    panop_eval_folder_dict['panoptic'] = os.path.join(panop_eval_root_folder, 'panoptic')
    panop_eval_folder_dict['debug_test'] = os.path.join(panop_eval_root_folder, 'debug_test')
    panop_eval_folder_dict['logger_eval'] = os.path.join(panop_eval_root_folder, 'logger_eval')
    panop_eval_folder_dict['tensorboard'] = os.path.join(panop_eval_root_folder, 'tensorboard')
    panop_eval_folder_dict['sem_vis'] = os.path.join(panop_eval_root_folder, 'sem_vis')
    panop_eval_folder_dict['pan_vis'] = os.path.join(panop_eval_root_folder, 'pan_vis')
    panop_eval_folder_dict['ins_vis'] = os.path.join(panop_eval_root_folder, 'ins_vis')

    if not os.path.exists(panop_eval_folder_dict['semantic']):
        os.makedirs(panop_eval_folder_dict['semantic'])
        os.makedirs(panop_eval_folder_dict['instance'])
        os.makedirs(panop_eval_folder_dict['panoptic'])
        os.makedirs(panop_eval_folder_dict['debug_test'])
        os.makedirs(panop_eval_folder_dict['logger_eval'])
        os.makedirs(panop_eval_folder_dict['sem_vis'])
        os.makedirs(panop_eval_folder_dict['pan_vis'])
        os.makedirs(panop_eval_folder_dict['ins_vis'])
    logger.info('')
    logger.info('Panoptic evaluation results will be save in these locations:')
    logger.info('panoptic eval root folder: {}'.format(panop_eval_root_folder))
    logger.info(panop_eval_folder_dict['semantic'])
    logger.info(panop_eval_folder_dict['instance'])
    logger.info(panop_eval_folder_dict['panoptic'])
    logger.info(panop_eval_folder_dict['debug_test'])
    logger.info(panop_eval_folder_dict['logger_eval'])
    logger.info(panop_eval_folder_dict['tensorboard'])
    logger.info(panop_eval_folder_dict['sem_vis'])
    logger.info(panop_eval_folder_dict['pan_vis'])
    logger.info(panop_eval_folder_dict['ins_vis'])
    logger.info('')
    return panop_eval_folder_dict, panop_eval_root_folder


def set_nets_mode(cfg, model, discriminator, discriminator2nd, mode):
    if mode == 'eval':
        get_module(model, cfg.DISTRIBUTED).eval()
        if not cfg.TRAIN_ONLY_SOURCE and cfg.ENABLE_DISCRIMINATOR:
            get_module(discriminator, cfg.DISTRIBUTED).eval()
        if not cfg.TRAIN_ONLY_SOURCE and cfg.ENABLE_DISCRIMINATOR_2ND:
            get_module(discriminator2nd, cfg.DISTRIBUTED).eval()
    elif mode == 'train':
        get_module(model, cfg.DISTRIBUTED).train()
        if not cfg.TRAIN_ONLY_SOURCE and cfg.ENABLE_DISCRIMINATOR:
            get_module(discriminator, cfg.DISTRIBUTED).train()
        if not cfg.TRAIN_ONLY_SOURCE and cfg.ENABLE_DISCRIMINATOR_2ND:
            get_module(discriminator2nd, cfg.DISTRIBUTED).train()


def prob_2_entropy(prob):
    """ convert probabilistic prediction maps to weighted self-information maps
    """
    n, c, h, w = prob.size()
    return -torch.mul(prob, torch.log2(prob + 1e-30)) / np.log2(c)


# need to cross check this implementation with Danda
def disc_fwd_pass(cfg, disc, disc2nd, semantic_pred, center_pred=None, offset_pred=None, depth_pred=None, mode=1):
    '''
    if mode == 0: # not center_pred and not depth_pred:
        ...
    elif mode == 1: # not center_pred and depth_pred:
        ...
    elif mode == 2: # center_pred and not depth_pred:
        ...
    elif mode == 3: # center_pred and depth_pred:
        ...
    '''

    if cfg.ENABLE_DISCRIMINATOR_2ND and mode < 4:
        raise NotImplementedError('ctrl/train_panop_utils.py --> disc_fwd_pass() --> when cfg.ENABLE_DISCRIMINATOR_2ND=True then mode should be > 3!!')

    d_out = None
    d_out2nd = None
    if mode == 0:  # not center_pred and not depth_pred:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)))
    elif mode == 1:  # not center_pred and depth_pred: # DADA
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)) * depth_pred)
    elif mode == 2:  # center_pred and not depth_pred:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)) * center_pred)
    elif mode == 3:  # center_pred and depth_pred:
        t1 = prob_2_entropy(F.softmax(semantic_pred, dim=1)) * depth_pred
        t2 = prob_2_entropy(F.softmax(semantic_pred, dim=1)) * center_pred
        t3 = torch.cat((t1, t2), dim=1)
        d_out = disc(t3)
    elif mode == 4:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)))
        d_out2nd = disc2nd(depth_pred)
    elif mode == 5:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)))
        d_out2nd = disc2nd(center_pred)
    elif mode == 6:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)))
        d_out2nd = disc2nd(depth_pred * center_pred)
    elif mode == 7:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)) * depth_pred)
        d_out2nd = disc2nd(prob_2_entropy(F.softmax(semantic_pred, dim=1)) * center_pred)
    elif mode == 8:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)))
        d_out2nd = disc2nd(torch.cat((depth_pred, center_pred), dim=1))
    elif mode == 9:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)))
        d_out2nd = disc2nd(torch.cat((depth_pred, center_pred, offset_pred), dim=1))
    elif mode == 10:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)) * depth_pred)
        d_out2nd = disc2nd(torch.cat((depth_pred, center_pred, offset_pred), dim=1))
    elif mode == 11:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)) * depth_pred)
        d_out2nd = disc2nd(torch.cat((prob_2_entropy(F.softmax(semantic_pred, dim=1)), center_pred, offset_pred), dim=1))
    elif mode == 12:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)))
        d_out2nd = disc2nd(torch.cat((center_pred, offset_pred), dim=1))
    elif mode == 13:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)))
        d_out2nd = disc2nd(torch.cat(((depth_pred * center_pred), offset_pred), dim=1))
    elif mode == 14:
        d_out = disc(prob_2_entropy(F.softmax(semantic_pred, dim=1)) * depth_pred)
        d_out2nd = disc2nd(torch.cat(((depth_pred * center_pred), offset_pred), dim=1))

    return d_out, d_out2nd


def get_instance_feat(sem, instance, DEVICE):
    _, C, _, _ = sem.shape
    sem = sem.view(1, C, -1)  # (1, 16, 512, 1024) --> (1, 16, 512*1024)
    instIds = instance.unique()
    num_inst = len(instIds)
    inst_feat = torch.zeros((num_inst, C))
    inst_feat = inst_feat.to(DEVICE)
    # if num_inst == 1:
    #     mask = instance == instIds[0]
    #     mask = mask.repeat(1, C, 1, 1)
    #     mask = mask.view(1, C, -1)
    #     inst_feat[0, :] = masked_mean_pooling(sem, mask, dim=2)
    # else:
    idx = 0
    for id in instIds:
        mask = instance == id
        mask = mask.repeat(1, C, 1, 1)
        mask = mask.view(1, C, -1)
        inst_feat[idx, :] = masked_mean_pooling(sem, mask, dim=2)
        idx += 1
    return inst_feat


def pool_semantic_feat_based_on_inst_pred(cfg, instance, sem, center, depth, DEVICE, i_iter):
    if cfg.APPROACH_DANDA_CONCAT_PE:
        height = sem.shape[2]
        width = sem.shape[3]
        k = torch.tensor([[width, 0, (width / 2)], [0, width, height / 2], [0, 0, 1]])
        k = k.to(DEVICE)
        # k_inv = torch.linalg.inv(k)
        k_inv = torch.inverse(k)
        instIds = instance.unique()
        num_inst = len(instIds)
        # if num_inst > 50:
        #     num_inst = 50
        y_coord = torch.arange(height, dtype=sem.dtype, device=sem.device).repeat(1, width, 1).transpose(1, 2)
        x_coord = torch.arange(width, dtype=sem.dtype, device=sem.device).repeat(1, height, 1)
        x_coord.requires_grad = False
        y_coord.requires_grad = False
        idx = 0
        im_coord = torch.zeros((3, num_inst))
        pos_enc = torch.zeros((3, num_inst))
        z_depth = torch.zeros((3, num_inst))
        k_inv = k_inv.to(DEVICE)
        im_coord = im_coord.to(DEVICE)
        z_depth = z_depth.to(DEVICE)
        depth = (65536.0 / depth) - 1
        depth = depth.view(-1)
        depth = depth / (depth.sum() + 1e-10)
        depth = depth.view(1, 1, height, width)
        for id in instIds:
            mask = instance == id
            mask = mask.unsqueeze(0)
            w = center * mask
            w = w.view(-1)
            w = w / (w.sum() + 1e-10)
            w = w.view(height, width)
            w = w.unsqueeze(0)
            x_c = x_coord * w
            y_c = y_coord * w
            # TODO: make sure that these x_c and y_c are matched with the centers you get from the non-differentiable module
            x_c = int(x_c.sum())
            y_c = int(y_c.sum())
            if x_c < 0:
                x_c = 0
            if x_c > width - 1:
                x_c = int(width - 1)
            if y_c < 0:
                y_c = 0
            if y_c > height - 1:
                y_c = int(height - 1)
            z_c = depth[:, :, y_c, x_c]
            z_c = z_c.item()
            im_coord[0, idx] = x_c
            im_coord[1, idx] = y_c
            im_coord[2, idx] = 1
            z_depth[:, idx] = z_c
            idx += 1
        world_cord = torch.matmul(k_inv, im_coord)
        world_cord = world_cord * z_depth
    # mean pool semantic feature as per the instance mask
    instance = instance.to(DEVICE)
    inst_feat = get_instance_feat(sem, instance, DEVICE)  # (num_instances, 16) # num_instances=seq_length, 16=embedding_dim
    # concat position coord with feature
    inst_feat_new = None
    if cfg.APPROACH_DANDA_CONCAT_PE:
        _, C, _, _ = sem.shape
        inst_feat_pe = torch.zeros((num_inst, C + 3)).to(DEVICE)  # TODO
        # inst_feat_pe = torch.zeros((num_inst, C + 4)).to(DEVICE)  # TODO
        for i in range(num_inst):
            inst_feat_pe[i, :C] = inst_feat[i, :]
            inst_feat_pe[i, C:C + 3] = world_cord[:, i]
        inst_feat_new = inst_feat_pe
    # else:
    #     inst_feat = inst_feat.unsqueeze(1)  # (num_instances, 1, 16) : (S, N, E) S: seq length, N: batchsize, E: feature dim
    #     inst_feat_new = inst_feat
    # if i_iter % 10 == 0:
    #     print(inst_feat_new.shape)
    return inst_feat_new


def compute_hungarian_based_loss(cfg, DEVICE, inst_feat, memory_weights, hungarian_based_triplet_loss, i_iter):
    # fact = 1000000000
    # computing the pairwise distance
    dist_matrix = torch.cdist(inst_feat.unsqueeze(0), memory_weights.unsqueeze(0), p=2)
    dist_matrix = dist_matrix.squeeze(0).cpu().detach().numpy()
    # Hungarian Matching
    rids1, cids1 = solve_dense(dist_matrix)
    #  torch.topk requires a tensor, so convert to tensor
    dist_matrix = torch.from_numpy(dist_matrix)
    # reserve mem to store the positives and negatives for triplet loss
    memory_weights_pos = torch.zeros(inst_feat.shape).to(DEVICE)
    memory_weights_neg = torch.zeros(inst_feat.shape).to(DEVICE)
    num_inst = inst_feat.shape[0]
    for i in range(num_inst):
        # collecting the positives
        memory_weights_pos[i, :] = memory_weights[cids1[i], :]
        # collecting the negatives
        # finding the two minimum values alow the i-th row of the distance matrix
        min_val_2, min_idx_2 = torch.topk(-dist_matrix[i, :], 2)
        neg_idx = min_idx_2[0]
        # if the index of the minimum equals to the matched idx cids1[i]
        # then assign the 2nd minimum as the negative sample
        if neg_idx == cids1[i]:
            neg_idx = min_idx_2[1]
        memory_weights_neg[i, :] = memory_weights[neg_idx, :]
    # normalize
    # inst_feat
    inst_feat_norm = torch.norm(inst_feat, p=2, dim=1).detach()
    inst_feat_norm = inst_feat_norm.unsqueeze(dim=1)
    inst_feat = inst_feat.div(inst_feat_norm.expand_as(inst_feat))
    # memory_weights_pos
    memory_weights_pos_norm = torch.norm(memory_weights_pos, p=2, dim=1).detach()
    memory_weights_pos_norm = memory_weights_pos_norm.unsqueeze(dim=1)
    memory_weights_pos = memory_weights_pos.div(memory_weights_pos_norm.expand_as(memory_weights_pos))
    # memory_weights_neg
    memory_weights_neg_norm = torch.norm(memory_weights_neg, p=2, dim=1).detach()
    memory_weights_neg_norm = memory_weights_neg_norm.unsqueeze(dim=1)
    memory_weights_neg = memory_weights_neg.div(memory_weights_neg_norm.expand_as(memory_weights_neg))
    # computing the triplet loss
    loss = hungarian_based_triplet_loss(inst_feat, memory_weights_pos, memory_weights_neg)
    # if i_iter % 10 == 0:
    #     inst_feat_norm = torch.norm(memory_weights[:10, :], p=2, dim=1).detach()
    #     print(inst_feat_norm)
    #     t = torch.cuda.get_device_properties(0).total_memory
    #     r = torch.cuda.memory_reserved(0)
    #     a = torch.cuda.memory_allocated(0)
    #     f = r - a  # free inside reserved
    #     print('total_memory: {} GB;  memory_reserved: {} GB; memory_allocated: {} GB; free mem: {} GB'.format(t/fact, r/fact, a/fact, f/fact))
    return loss


# def disc_fwd_pass_danda(cfg, disc, disc2nd, instance, sem, center, depth, DEVICE, memory_weights):
#     '''
#      instance: {Tensor: (1, 1024, 2048)}: instance mask computed using a non-differentiable moudle
#     center: {Tensor: (1, 1024, 2048)}:  center  predictions (heatmap)
#     depth: {Tensor: (1, 1024, 2048)}: depth predictions (heatmap)
#     '''
#     d_out = 0
#     d_out2nd = 0
#
#     if cfg.APPROACH_DANDA_CONCAT_PE:
#         height = sem.shape[2]
#         width = sem.shape[3]
#         k = torch.tensor([[width, 0, (width / 2)], [0, width, height / 2], [0, 0, 1]])
#         k = k.to(DEVICE)
#         # k_inv = torch.linalg.inv(k)
#         k_inv = torch.inverse(k)
#         instIds = instance.unique()
#         num_inst = len(instIds)
#         y_coord = torch.arange(height, dtype=sem.dtype, device=sem.device).repeat(1, width, 1).transpose(1, 2)
#         x_coord = torch.arange(width, dtype=sem.dtype, device=sem.device).repeat(1, height, 1)
#         x_coord.requires_grad = False
#         y_coord.requires_grad = False
#         idx = 0
#         im_coord = torch.zeros((3, num_inst))
#         pos_enc = torch.zeros((3, num_inst))
#         z_depth = torch.zeros((3, num_inst))
#         k_inv = k_inv.to(DEVICE)
#         im_coord = im_coord.to(DEVICE)
#         z_depth = z_depth.to(DEVICE)
#         depth = (65536.0 / depth) - 1
#         depth = depth.view(-1)
#         depth = depth / (depth.sum() + 1e-10)
#         depth = depth.view(1, 1, height, width)
#
#         for id in instIds:
#             mask = instance == id
#             mask = mask.unsqueeze(0)
#             w = center * mask
#             w = w.view(-1)
#             w = w / (w.sum() + 1e-10)
#             w = w.view(height, width)
#             w = w.unsqueeze(0)
#             x_c = x_coord * w
#             y_c = y_coord * w
#             # TODO: make sure that these x_c and y_c are matched with the centers you get from the non-differentiable module
#             x_c = int(x_c.sum())
#             y_c = int(y_c.sum())
#             if x_c < 0:
#                 x_c = 0
#             if x_c > width-1:
#                 x_c = int(width-1)
#             if y_c < 0:
#                 y_c = 0
#             if y_c > height-1:
#                 y_c = int(height-1)
#             z_c = depth[:, :, y_c, x_c]
#             z_c = z_c.item()
#             im_coord[0, idx] = x_c
#             im_coord[1, idx] = y_c
#             im_coord[2, idx] = 1
#             z_depth[:, idx] = z_c
#             idx += 1
#
#         world_cord = torch.matmul(k_inv, im_coord)
#         world_cord = world_cord * z_depth
#
#     # mean pool semantic feature as per the instance mask
#     instance = instance.to(DEVICE)
#     inst_feat = get_instance_feat(sem, instance, DEVICE) # (num_instances, 16) # num_instances=seq_length, 16=embedding_dim
#     # concat position coord with feature
#     if cfg.APPROACH_DANDA_CONCAT_PE:
#         _, C, _, _ = sem.shape
#         inst_feat_pe = torch.zeros((num_inst, C+4)).to(DEVICE)
#         for i in range(num_inst):
#             inst_feat_pe[i, :C] = inst_feat[i, :]
#             inst_feat_pe[i, C:C+3] = world_cord[:, i]
#
#         if cfg.ENABLE_DISCRIMINATOR:
#             inst_feat_pe = inst_feat_pe.unsqueeze(1)  # (num_instances, 1, 20) : (S, N, E) S: seq length, N: batchsize, E: feature dim
#             d_out = disc(inst_feat_pe)
#
#     else:
#         inst_feat = inst_feat.unsqueeze(1)  # (num_instances, 1, 16) : (S, N, E) S: seq length, N: batchsize, E: feature dim
#         d_out = disc(inst_feat)
#
#     if cfg.ENABLE_DISCRIMINATOR_2ND:
#         d_out2nd = disc2nd(prob_2_entropy(F.softmax(sem, dim=1)) * depth)
#
#     return d_out, d_out2nd

def get_roi_boxes(instance, h, w):
    instIds = instance.unique()
    num_inst = len(instIds)
    boxes = torch.zeros((num_inst, 5))
    # initialize default box to be the one which has same dimension as the image dim
    # in case there is no instance then we pool the feature of the entire image
    # and pass to the discriminator
    boxes[:, 0] = 0
    boxes[:, 1] = 0
    boxes[:, 2] = 0
    boxes[:, 3] = w - 1
    boxes[:, 4] = h - 1
    bidx = 0
    for id in instIds:
        mask = instance == id
        mask = mask.squeeze(0)
        # bbox computation for a segment
        hor = mask.sum(dim=0)
        hor_idx = torch.nonzero(hor)
        x1 = hor_idx[0, 0].item()
        # width = hor_idx[-1] - x1 + 1
        x2 = hor_idx[-1, 0].item()

        ver = mask.sum(dim=1)
        ver_idx = torch.nonzero(ver)
        y1 = ver_idx[0, 0].item()
        y2 = ver_idx[-1, 0].item()
        # height = vert_idx[-1] - y + 1
        if x1 >= 0 and x2 > x1 and y1 >= 0 and y2 > y1 and x2 <= w and y2 <= h:
            boxes[bidx, 0] = 0
            boxes[bidx, 1] = x1
            boxes[bidx, 2] = y1
            boxes[bidx, 3] = x2
            boxes[bidx, 4] = y2
            # print(boxes)
        else:
            continue
    bidx += 1
    return boxes


def disc_fwd_pass_danda_old(disc, instance, sem, center, depth, DEVICE, roi_window_size):
    '''
     instance: {Tensor: (1, 1024, 2048)}: instance mask computed using a non-differentiable moudle
    center: {Tensor: (1, 1024, 2048)}:  center  predictions (heatmap)
    depth: {Tensor: (1, 1024, 2048)}: depth predictions (heatmap)
    '''
    height = sem.shape[2]
    width = sem.shape[3]
    boxes = get_roi_boxes(instance, height, width)
    # boxes = boxes.to(dtype=sem.dtype)
    boxes = boxes.to(DEVICE)
    # TODO: what should be the window size for pooling?
    # TODO: in object detection the pooling is done on the downsampled feature map
    # TODO: which is H/16 and width/16, here we work on the original image dimension
    rws = roi_window_size
    roi_feats = torchvision.ops.roi_align(sem, boxes, output_size=(rws, rws))  # roi_feat: K x C x 112 x 112
    # d_out = disc(roi_feats)
    # return d_out
    k = torch.tensor([[width, 0, (width / 2)], [0, width, height / 2], [0, 0, 1]])
    k = k.to(DEVICE)
    k_inv = torch.linalg.inv(k)
    instIds = instance.unique()
    num_inst = len(instIds)
    y_coord = torch.arange(height, dtype=sem.dtype, device=sem.device).repeat(1, width, 1).transpose(1, 2)
    x_coord = torch.arange(width, dtype=sem.dtype, device=sem.device).repeat(1, height, 1)
    x_coord.requires_grad = False
    y_coord.requires_grad = False
    idx = 0
    im_coord = torch.tensor((3, num_inst))
    z_depth = torch.tensor((3, num_inst))
    for id in instIds:
        mask = instance == id
        mask = mask.unsqueeze(0)
        w = center[mask]  # center heatmap with scores
        # w = w.view(-1)
        # w = torch.nn.functional.normalize(w, dim=0)
        w = w.view(height, width)
        w = w / (w.sum() + 1e-10)
        x_c = x_coord * w
        y_c = y_coord * w
        z_c = depth[x_c, y_c]
        # TODO: make sure that these x_c and y_c are matched with the centers you get from the non-differentiable module
        im_coord[0, idx] = x_c.sum()
        im_coord[1, idx] = y_c.sum()
        im_coord[2, idx] = 1
        z_depth[:, idx] = z_c
        idx += 1
    # convert (x_c, y_c) to (X_c, Y_c) using the predicted depth
    world_cord = k_inv * im_coord * z_depth  # 3 x num_inst

    transformer_input_feat = None
    for id in range(num_inst):
        transformer_input_feat = [(roi_feats[idx, :], world_cord[:, idx])]
    d_out = disc(transformer_input_feat)
    return d_out